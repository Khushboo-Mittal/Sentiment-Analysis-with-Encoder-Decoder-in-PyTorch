{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial dataset information:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1006 entries, 0 to 1005\n",
      "Data columns (total 4 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   tweetID        1006 non-null   int64 \n",
      " 1   entity         1006 non-null   object\n",
      " 2   sentiment      1006 non-null   object\n",
      " 3   tweet_content  1002 non-null   object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 31.6+ KB\n",
      "None\n",
      "   tweetID       entity sentiment  \\\n",
      "0     2401  Borderlands  Positive   \n",
      "1     2401  Borderlands  Positive   \n",
      "2     2401  Borderlands  Positive   \n",
      "3     2401  Borderlands  Positive   \n",
      "4     2401  Borderlands  Positive   \n",
      "\n",
      "                                       tweet_content  \n",
      "0  im getting on borderlands and i will murder yo...  \n",
      "1  I am coming to the borders and I will kill you...  \n",
      "2  im getting on borderlands and i will kill you ...  \n",
      "3  im coming on borderlands and i will murder you...  \n",
      "4  im getting on borderlands 2 and i will murder ...  \n"
     ]
    }
   ],
   "source": [
    "# META DATA - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
    "\n",
    "    # Developer details: \n",
    "        # Name: Khushboo Mittal, Prachi Tavse, Tanisha Priya, Harshita Jangde \n",
    "        # Role: Architect\n",
    "    # Version:\n",
    "        # Version: V 1.0 (24 October 2024)\n",
    "            # Developers: Khushboo Mittal, Prachi Tavse, Tanisha Priya, Harshita Jangde \n",
    "            # Unit test: Pass\n",
    "            # Integration test: Pass\n",
    "     \n",
    "     # Description: This code snippet implements an Encoder-Decoder model for Natural Language Processing (NLP) \n",
    "     # tasks, focusing on sequence-to-sequence transformation. The model uses embedding layers for input \n",
    "     # encoding, followed by an RNN (GRU) architecture in the encoder to capture sequential dependencies. \n",
    "     # The decoder generates output sequences based on the encoded context. Applications include text translation, \n",
    "     # summarization, and question answering, with support for custom vocabularies and tokenization.\n",
    "\n",
    "# CODE - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
    "\n",
    "    # Dependency: \n",
    "        # Environment:     \n",
    "            # Python: 3.11.0\n",
    "            # torch: 2.1.0\n",
    "            # torchtext: 0.16.0\n",
    "            # sklearn: 1.5.0\n",
    "            # pandas: 2.2.2\n",
    "            # matplotlib: 3.8.2\n",
    "\n",
    "import pandas as pd  # Import pandas for data manipulation\n",
    "from sklearn.model_selection import train_test_split  # Import function for splitting datasets\n",
    "import torch  # Import PyTorch for tensor operations\n",
    "from torchtext.data.utils import get_tokenizer  # Import tokenizer utility from torchtext\n",
    "from torchtext.vocab import build_vocab_from_iterator  # Import function to build vocabulary from text\n",
    "from torch.utils.data import Dataset, DataLoader  # Import classes for dataset handling\n",
    "from torch.nn.utils.rnn import pad_sequence  # Import function to pad sequences for batching\n",
    "from sklearn.metrics import accuracy_score, classification_report # Import the function for evaluation metrics\n",
    "import matplotlib.pyplot as plt # Import matplotlib for visualisation\n",
    "\n",
    "# Define column names for the dataset\n",
    "column_names = ['tweetID', 'entity', 'sentiment', 'tweet_content']\n",
    "\n",
    "# Load the dataset with specified column names; no header in the CSV\n",
    "df = pd.read_csv('../Data/twitter_sentiment_analysis.csv', names=column_names, header=None)\n",
    "\n",
    "print(\"Initial dataset information:\")\n",
    "print(df.info())  # Show basic info\n",
    "print(df.head())  # Show first few rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset after preprocessing:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1006 entries, 0 to 1005\n",
      "Data columns (total 3 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   tweetID        1006 non-null   int64 \n",
      " 1   sentiment      1006 non-null   int64 \n",
      " 2   tweet_content  1006 non-null   object\n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 23.7+ KB\n",
      "None\n",
      "   tweetID  sentiment                                      tweet_content\n",
      "0     2401          1  im getting on borderlands and i will murder yo...\n",
      "1     2401          1  I am coming to the borders and I will kill you...\n",
      "2     2401          1  im getting on borderlands and i will kill you ...\n",
      "3     2401          1  im coming on borderlands and i will murder you...\n",
      "4     2401          1  im getting on borderlands 2 and i will murder ...\n"
     ]
    }
   ],
   "source": [
    "df = df[['tweetID', 'sentiment', 'tweet_content']]  # Select only relevant columns\n",
    "# Basic preprocessing of sentiment labels and tweet content\n",
    "df['sentiment'] = df['sentiment'].map({'Positive': 1, 'Negative': 0, 'Neutral': 2, 'Irrelevant': 3})  # Encode sentiments as integers\n",
    "df['tweet_content'] = df['tweet_content'].astype(str)  # Ensure tweet content is of string type\n",
    "df.dropna(subset=['sentiment'], inplace=True)  # Remove rows with missing sentiment values\n",
    "\n",
    "print(\"Dataset after preprocessing:\")\n",
    "print(df.info())\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 855, Validation set size: 75, Test set size: 76\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training, validation, and test sets\n",
    "train_df, test_df = train_test_split(df, test_size=0.15, random_state=42)  # 15% of data for testing\n",
    "valid_df, test_df = train_test_split(test_df, test_size=0.5, random_state=42)  # Split remaining data into valid and test sets\n",
    "\n",
    "print(f\"Training set size: {len(train_df)}, Validation set size: {len(valid_df)}, Test set size: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 2250\n",
      "Most common tokens:\n",
      "[('.', 2975), ('i', 726), ('the', 696), ('borderlands', 602), ('!', 547), ('to', 469), ('and', 468), (',', 434), ('a', 409), (\"'\", 374)]\n",
      "\n",
      "Least common tokens:\n",
      "[('hive', 1), ('counting', 1), ('dubbed', 1), ('single', 1), ('eight', 1), ('mine', 1), ('underpaying', 1), ('badly', 1), ('sweat', 1), ('planning', 1)]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenization using a basic English tokenizer\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "# Function to yield tokens for vocabulary creation\n",
    "def yield_tokens(data_iter):\n",
    "    for tweet in data_iter:  # Iterate through each tweet\n",
    "        yield tokenizer(tweet)  # Tokenize and yield tokens\n",
    "\n",
    "# Build vocabulary from the tokenized tweets, including a special unknown token\n",
    "vocab = build_vocab_from_iterator(yield_tokens(df['tweet_content']), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])  # Set default index for unknown words to the unknown token\n",
    "\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n",
    "\n",
    "# Display the most common and least common tokens in the vocabulary\n",
    "token_freq = vocab.get_stoi()  # Get the token-to-index mapping (vocabulary)\n",
    "print(\"Most common tokens:\")\n",
    "# Count token frequencies by iterating through the dataset (you can also use collections.Counter)\n",
    "token_counts = {}\n",
    "for tweet in df['tweet_content']:\n",
    "    for word in tokenizer(tweet):\n",
    "        token_counts[word] = token_counts.get(word, 0) + 1\n",
    "sorted_token_counts = sorted(token_counts.items(), key=lambda x: -x[1])\n",
    "\n",
    "# Show top 10 most frequent and least frequent tokens\n",
    "print(sorted_token_counts[:10])  # Top 10 most frequent tokens\n",
    "print(\"\\nLeast common tokens:\")\n",
    "print(sorted_token_counts[-10:])  # Least frequent tokens\n",
    "\n",
    "# Create a histogram showing token frequencies (useful for analyzing long-tail distributions)\n",
    "frequencies = list(token_counts.values())\n",
    "plt.hist(frequencies, bins=50, log=True)\n",
    "plt.title(\"Token Frequency Distribution\")\n",
    "plt.xlabel(\"Frequency\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: im getting on borderlands and i will murder you all ,\n",
      "Tokenized: ['im', 'getting', 'on', 'borderlands', 'and', 'i', 'will', 'murder', 'you', 'all', ',']\n",
      "\n",
      "Original: I am coming to the borders and I will kill you all,\n",
      "Tokenized: ['i', 'am', 'coming', 'to', 'the', 'borders', 'and', 'i', 'will', 'kill', 'you', 'all', ',']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Show an example of tokenization for a few sample sentences\n",
    "sample_tweets = df['tweet_content'][:2]\n",
    "for tweet in sample_tweets:\n",
    "    print(f\"Original: {tweet}\")\n",
    "    print(f\"Tokenized: {tokenizer(tweet)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare a custom dataset class for loading tweets and their sentiments\n",
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe  # Store the DataFrame\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)  # Return the size of the dataset\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tweet = self.dataframe.iloc[idx]['tweet_content']  # Get the tweet content\n",
    "        sentiment = self.dataframe.iloc[idx]['sentiment']  # Get the sentiment label\n",
    "        return torch.tensor(vocab(tokenizer(tweet))), torch.tensor(sentiment)  # Return tokenized tweet and sentiment as tensors\n",
    "\n",
    "# Function to collate data into batches for training\n",
    "def collate_fn(batch):\n",
    "    src_batch, trg_batch = zip(*batch)  # Unzip the batch into tweet and sentiment tensors\n",
    "    src_batch = pad_sequence(src_batch, padding_value=vocab[\"<pad>\"], batch_first=True)  # Pad the tweet sequences\n",
    "    trg_batch = torch.stack(trg_batch)  # Stack sentiment labels into a tensor\n",
    "    return src_batch, trg_batch  # Return padded tweets and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader Insights\n",
      "Training Dataset: 855 samples, Batch Size: 32, Shuffling: True\n",
      "Validation Dataset: 75 samples, Batch Size: 32, Shuffling: False\n",
      "Test Dataset: 76 samples, Batch Size: 32, Shuffling: False\n",
      "\n",
      "Train DataLoader - First Batch:\n",
      "(tensor([[  23,    2,  350,  ...,    0,    0,    0],\n",
      "        [  96,  702,   52,  ...,    0,    0,    0],\n",
      "        [ 622,  155,  190,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 355,   25,  135,  ...,    0,    0,    0],\n",
      "        [ 109, 1830,  293,  ...,    0,    0,    0],\n",
      "        [ 219,  191,  224,  ...,    0,    0,    0]]), tensor([1, 3, 2, 0, 2, 0, 1, 3, 1, 2, 3, 3, 1, 3, 0, 1, 3, 1, 1, 3, 1, 1, 2, 2,\n",
      "        1, 0, 2, 0, 3, 1, 1, 1]))\n",
      "\n",
      "Validation DataLoader - First Batch:\n",
      "(tensor([[  22,    4,   22,  ...,    0,    0,    0],\n",
      "        [1327,   23,    2,  ...,    0,    0,    0],\n",
      "        [   1,  188,   60,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [  62,    9,  142,  ...,    0,    0,    0],\n",
      "        [ 128,   50,    2,  ...,    0,    0,    0],\n",
      "        [ 113,    1,    2,  ...,    0,    0,    0]]), tensor([1, 1, 1, 1, 1, 2, 2, 0, 0, 1, 0, 1, 1, 2, 0, 1, 3, 3, 0, 1, 0, 0, 1, 1,\n",
      "        2, 1, 1, 1, 1, 1, 0, 1]))\n",
      "\n",
      "Test DataLoader - First Batch:\n",
      "(tensor([[ 243,    3,   63,  ...,    0,    0,    0],\n",
      "        [   3,  274,  434,  ...,    0,    0,    0],\n",
      "        [ 233,    6,  469,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [  38,  344,    2,  ...,    0,    0,    0],\n",
      "        [ 145,   31,    2,  ...,    0,    0,    0],\n",
      "        [ 590, 1403, 1426,  ...,    0,    0,    0]]), tensor([1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 2, 1, 0, 1, 0, 1, 2, 2, 1, 1, 0, 1,\n",
      "        1, 1, 3, 0, 1, 1, 2, 2]))\n"
     ]
    }
   ],
   "source": [
    "def show_batch(dataloader, name):\n",
    "    print(f\"\\n{name} - First Batch:\")\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        print(batch)  # Replace with specific unpacking logic if batch contains nested structures\n",
    "        if i == 0:  # Show only the first batch\n",
    "            break\n",
    "        \n",
    "# Create dataset instances for training, validation, and test sets\n",
    "train_dataset = TweetDataset(train_df)\n",
    "valid_dataset = TweetDataset(valid_df)\n",
    "test_dataset = TweetDataset(test_df)\n",
    "\n",
    "# Create DataLoader instances for each dataset to handle batching and shuffling\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "print(\"DataLoader Insights\")\n",
    "print(f\"Training Dataset: {len(train_dataset)} samples, Batch Size: 32, Shuffling: True\")\n",
    "print(f\"Validation Dataset: {len(valid_dataset)} samples, Batch Size: 32, Shuffling: False\")\n",
    "print(f\"Test Dataset: {len(test_dataset)} samples, Batch Size: 32, Shuffling: False\")\n",
    "\n",
    "# Displaying a batch sample for validation\n",
    "show_batch(train_dataloader, \"Train DataLoader\")\n",
    "show_batch(valid_dataloader, \"Validation DataLoader\")\n",
    "show_batch(test_dataloader, \"Test DataLoader\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn  # Import PyTorch's neural network module\n",
    "\n",
    "# Define the Encoder class\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hidden_dim):\n",
    "        super(Encoder, self).__init__()  # Initialize the parent class\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)  # Embedding layer to convert input tokens to vectors\n",
    "        self.rnn = nn.GRU(emb_dim, hidden_dim, batch_first=True)  # GRU layer with input dimension as embedding size\n",
    "\n",
    "    def forward(self, src):\n",
    "        embedded = self.embedding(src)  # Convert input token indices to embeddings\n",
    "        outputs, hidden = self.rnn(embedded)  # Pass embeddings through the RNN\n",
    "        return hidden  # Return the hidden state for further processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Decoder generates predictions (sentiment labels) based on the hidden state provided by the encoder and the current input token.\n",
    "parameters of intit:\n",
    "\n",
    "output_dim: The size of the output vocabulary (number of unique sentiment labels).\n",
    "\n",
    "emb_dim: The dimension of the embedding vectors (same as in the encoder).\n",
    "\n",
    "hidden_dim: The number of features in the hidden state (same as in the encoder)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Decoder class\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hidden_dim):\n",
    "        super(Decoder, self).__init__()  # Initialize the parent class\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)  # Embedding layer for output tokens\n",
    "        self.rnn = nn.GRU(emb_dim, hidden_dim, batch_first=True)  # GRU layer for decoding\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)  # Fully connected layer to convert hidden states to output logits\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        input = input.unsqueeze(1)  # Reshape input for RNN: (batch_size, 1)\n",
    "        embedded = self.embedding(input)  # Convert input token indices to embeddings\n",
    "        output, hidden = self.rnn(embedded, hidden)  # Pass embeddings and hidden state through the RNN\n",
    "        prediction = self.fc(output.squeeze(1))  # Convert RNN output to predictions, shape: (batch_size, output_dim)\n",
    "        return prediction, hidden  # Return predictions and the new hidden state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()  # Initialize the parent class\n",
    "        self.encoder = encoder  # Assign the encoder model\n",
    "        self.decoder = decoder  # Assign the decoder model\n",
    "\n",
    "    def forward(self, src):\n",
    "        hidden = self.encoder(src)  # Encode the source sequence and obtain the final hidden state\n",
    "        input = torch.zeros(src.size(0), dtype=torch.long).to(src.device)  # Create a start token (shape: batch_size)\n",
    "        output, _ = self.decoder(input, hidden)  # Decode using the start token and the hidden state from the encoder\n",
    "        return output  # Return the decoder's output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.3033\n",
      "Epoch 2/10, Loss: 1.3020\n",
      "Epoch 3/10, Loss: 1.2866\n",
      "Epoch 4/10, Loss: 1.2761\n",
      "Epoch 5/10, Loss: 1.2017\n",
      "Epoch 6/10, Loss: 0.9047\n",
      "Epoch 7/10, Loss: 0.6211\n",
      "Epoch 8/10, Loss: 0.3668\n",
      "Epoch 9/10, Loss: 0.1681\n",
      "Epoch 10/10, Loss: 0.1177\n",
      "\n",
      "Final Evaluation Metrics:\n",
      "Training Accuracy: 0.9766\n",
      "Validation Accuracy: 0.8933\n",
      "Test Accuracy: 0.9079\n",
      "Training Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.99       161\n",
      "           1       0.95      1.00      0.97       350\n",
      "           2       1.00      0.94      0.97       250\n",
      "           3       1.00      0.97      0.98        94\n",
      "\n",
      "    accuracy                           0.98       855\n",
      "   macro avg       0.99      0.97      0.98       855\n",
      "weighted avg       0.98      0.98      0.98       855\n",
      "\n",
      "Validation Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.80      0.89        20\n",
      "           1       0.90      0.95      0.92        38\n",
      "           2       0.79      0.85      0.81        13\n",
      "           3       0.80      1.00      0.89         4\n",
      "\n",
      "    accuracy                           0.89        75\n",
      "   macro avg       0.87      0.90      0.88        75\n",
      "weighted avg       0.90      0.89      0.89        75\n",
      "\n",
      "Test Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      1.00      0.97        15\n",
      "           1       0.95      0.92      0.93        38\n",
      "           2       0.83      0.79      0.81        19\n",
      "           3       0.80      1.00      0.89         4\n",
      "\n",
      "    accuracy                           0.91        76\n",
      "   macro avg       0.88      0.93      0.90        76\n",
      "weighted avg       0.91      0.91      0.91        76\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Model initialization\n",
    "INPUT_DIM = len(vocab)  # Number of unique tokens in the vocabulary\n",
    "OUTPUT_DIM = 4  # Number of sentiment classes (0: Negative, 1: Positive, 2: Neutral, 3: Irrelevant)\n",
    "EMB_DIM = 100  # Dimensionality of the embedding layer\n",
    "HIDDEN_DIM = 256  # Number of hidden units in the RNN\n",
    "N_EPOCHS = 10  # Number of training epochs\n",
    "\n",
    "# Instantiate the encoder and decoder\n",
    "encoder = Encoder(INPUT_DIM, EMB_DIM, HIDDEN_DIM)  # Create an Encoder object\n",
    "decoder = Decoder(OUTPUT_DIM, EMB_DIM, HIDDEN_DIM)  # Create a Decoder object\n",
    "model = Seq2Seq(encoder, decoder)  # Create a Seq2Seq model\n",
    "\n",
    "# Define the main training function, which calculates metrics at the end of training\n",
    "def train_model(model, train_dataloader, valid_dataloader, test_dataloader, num_epochs, criterion, optimizer):\n",
    "    model.train()  # Set the model to training mode\n",
    "    \n",
    "    for epoch in range(num_epochs):  # Loop through each epoch\n",
    "        total_loss = 0  # Variable to accumulate total loss for this epoch\n",
    "        \n",
    "        for src, trg in train_dataloader:  # Iterate through batches in the training DataLoader\n",
    "            optimizer.zero_grad()  # Reset gradients\n",
    "            \n",
    "            # Forward pass through the model\n",
    "            output = model(src)  # Compute model predictions for the current batch\n",
    "            loss = criterion(output, trg)  # Compute loss for this batch\n",
    "            total_loss += loss.item()  # Accumulate batch loss\n",
    "            \n",
    "            # Backward pass and parameter update\n",
    "            loss.backward()  # Calculate gradients\n",
    "            optimizer.step()  # Update model parameters\n",
    "\n",
    "        # Print average loss for the epoch\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(train_dataloader):.4f}\")\n",
    "\n",
    "    # Calculate and print metrics after the training loop ends\n",
    "    print(\"\\nFinal Evaluation Metrics:\")\n",
    "\n",
    "    # Calculate metrics on training set\n",
    "    train_predictions, train_true_labels = evaluate(model, train_dataloader)\n",
    "    train_accuracy = accuracy_score(train_true_labels, train_predictions)\n",
    "    print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "\n",
    "    # Calculate metrics on validation set\n",
    "    val_predictions, val_true_labels = evaluate(model, valid_dataloader)\n",
    "    val_accuracy = accuracy_score(val_true_labels, val_predictions)\n",
    "    print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "    # Calculate metrics on test set\n",
    "    test_predictions, test_true_labels = evaluate(model, test_dataloader)\n",
    "    test_accuracy = accuracy_score(test_true_labels, test_predictions)\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    \n",
    "    print(\"Training Classification Report:\\n\", classification_report(train_true_labels, train_predictions))\n",
    "    print(\"Validation Classification Report:\\n\", classification_report(val_true_labels, val_predictions))\n",
    "    print(\"Test Classification Report:\\n\", classification_report(test_true_labels, test_predictions))\n",
    "\n",
    "# Define evaluation function for dataloaders\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    predictions, true_labels = [], []  # Lists to store predictions and true labels\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for src, trg in dataloader:  # Iterate through batches in the DataLoader\n",
    "            output = model(src)  # Forward pass through the model\n",
    "            predictions.extend(output.argmax(dim=1).cpu().numpy())  # Get predicted classes\n",
    "            true_labels.extend(trg.cpu().numpy())  # Add true labels to list\n",
    "    \n",
    "    return predictions, true_labels  # Return lists of predictions and true labels\n",
    "\n",
    "# Training configuration\n",
    "criterion = nn.CrossEntropyLoss()  # Loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  # Optimizer with learning rate\n",
    "\n",
    "# Run the training process\n",
    "train_model(model, train_dataloader, valid_dataloader, test_dataloader, N_EPOCHS, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Prediction System\n",
      "Enter a sentence to analyze its sentiment or type 'exit' to quit. \n",
      "\n",
      "Input statement: Thank you\n",
      "Predicted Sentiment: Positive\n",
      "\n",
      "Exiting the sentiment prediction system. Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# Function to preprocess and predict sentiment for a single user input\n",
    "def predict_sentiment(model, sentence, tokenizer, vocab):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    # Tokenize and convert input sentence to vocabulary indices\n",
    "    tokens = tokenizer(sentence)  # Tokenize the sentence\n",
    "    token_indices = torch.tensor([vocab(tokens)]).to(next(model.parameters()).device)  # Convert tokens to indices\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation for inference\n",
    "        output = model(token_indices)  # Forward pass through the model\n",
    "        prediction = output.argmax(dim=1).item()  # Get the class index with the highest probability\n",
    "\n",
    "    # Map class index to sentiment label\n",
    "    sentiment_labels = {0: \"Negative\", 1: \"Positive\", 2: \"Neutral\", 3: \"Irrelevant\"}\n",
    "    return sentiment_labels[prediction]\n",
    "\n",
    "# Example interaction\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Sentiment Prediction System\")\n",
    "    print(\"Enter a sentence to analyze its sentiment or type 'exit' to quit. \\n\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"Your sentence: \")\n",
    "        if user_input.lower() == 'exit':  # Exit condition\n",
    "            print(\"Exiting the sentiment prediction system. Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        # Make a prediction\n",
    "        sentiment = predict_sentiment(model, user_input, tokenizer, vocab)\n",
    "        print(f\"Input statement: {user_input}\")\n",
    "        print(f\"Predicted Sentiment: {sentiment}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
